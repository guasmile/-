{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、网页源代码获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取url，转译。定位关键词。多页处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.baidu.com/'\n",
    "res = requests.get(url)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.baidu.com/s?wd=%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4&rsv_spt=1&rsv_iqid=0xf4c4e7080000216f&issp=1&f=8&rsv_bp=1&rsv_idx=2&ie=utf-8&tn=44048691_1_oem_dg&rsv_enter=1&rsv_dl=tb&rsv_sug3=8&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=1874&rsv_sug4=2644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&ie=utf-8&word=%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safa-ri 537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd=阿里巴巴'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, headers = headers).text\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # findall()函数的功能是在原始文本中寻找所有符合匹配规则的文本内容，其使用格式如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = 'hello 123 world'  #举例\n",
    "result = re.findall('\\d\\d\\d', content)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、非贪婪匹配之（.＊? ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“.”表示除了换行符外的任意字符，“*”表示0个或多个表达式。将“.”和“*”合在一起组成的匹配规则“.*”称为贪婪匹配。之所以叫贪婪匹配，是因为会匹配到过多的内容。如果再加上一个“? ”构成“.*? ”，就变成了非贪婪匹配，它能较精确地匹配到想要的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单来说，(.*? )用于获取文本A与文本B之间的内容，并不需要知道它的确切长度及格式，但是需要知道它在哪两个内容之间，其使用格式如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = '文本A百度新闻文本B,新浪文本A百新闻文本B文本A'  #举例\n",
    "p_source = '文本A(.*?)文本B'\n",
    "source = re.findall(p_source, res)\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、非贪婪匹配之．＊?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "．*？是用来做什么的呢？简单来说，.*?用于代替文本C和文本D之间的所有内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = '<h3>文本C<变化的网站信息>文本D新闻标题</h3>'\n",
    "p_title = '<h3>文本C.*?文本D(.*?)</h3>'\n",
    "title = re.findall(p_title, res)\n",
    "print(title)\n",
    "\n",
    "# 文本C和文本D之间为变化的网址，用．*？代表，需要提取的是文本D和</h3>之间的内容，用(.*? )代表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = '<h3 class=\"c-title\"><a href=\"网址\" data-click=\"{英文&数字}\"><em>阿里巴巴</em>代码竞赛现全球首位AI评委 能为代码质量打分</a>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_title = '<h3 class=\"c-title\">.*?>(.*?)</a>'\n",
    "title = re.findall(p_title, res)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_title = '<h3 class=\"c-title\"><a href=\"(.*?)\"'\n",
    "title = re.findall(p_title, res)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、修饰符re.S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的演示代码都没有考虑换行的情况，但实际的网页源代码里存在很多换行，而(.*? )和．*？无法自动匹配换行，如果遇到换行就不会继续匹配换行之后的内容了。此时就要用到下一小节的知识点：修饰符re.S。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = '''文本A\n",
    "\n",
    "百度新闻文本B'''\n",
    "\n",
    "p_source = '文本A(.*?)文本B'\n",
    "source = re.findall(p_source, res, re.S)  # 匹配规则，原始文本，修饰符\n",
    "print(source)\n",
    "\n",
    "# 获取的新闻标题包含换行符\\n和空格，可以利用1.4.3小节介绍的strip()函数清除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(source)):\n",
    "    source[i] = source[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、知识点补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sub()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub()函数中的sub是英文substitute（替换）的缩写，其格式为：re.sub(需要替换的内容，替换值，原字符串)。该函数主要用于清洗正则表达式获取到的内容，如之前获取到的有无效内容的新闻标题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = ['<em>阿里巴巴</em>代码竞赛现全球首位AI评委 能为代码质量打分']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title[0] = re.sub('<em>', '', title[0])\n",
    "title[0] = re.sub('</em>', '', title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方式的缺点是，得为每个要替换的字符串写一行替换代码，如果要替换的字符串有很多，工作量就会比较大。此时可以观察要替换的字符串，如果它们有类似的格式，就可以用sub()函数通过正则表达式进行批量替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title[0] = re.sub('<.*?>', '', title[0])  # <.*? >就表示任何<×××>形式的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中括号的用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中括号最主要的功能是使中括号里的内容不再有特殊含义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = '*华能信托'\n",
    "company1 = re.sub('[*]','', company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、百度新闻案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safa-ri 537.36'}\n",
    "url = 'https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&wd=阿里巴巴'\n",
    "res = requests.get(url, headers = headers).text\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "p_info = '<!--s-text-->(.*?)<!--s-text-->'\n",
    "info = re.findall(p_info, res, re.S)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&word=%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(info[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_title = '(.*?)<!--/s-text--></a></h3><div class=\"c-row c-gap-top-small\">'\n",
    "title = re.findall(p_title, info, re.S)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_news = '<span class=\"c-color-gray c-font-normal c-gap-right\">(.*?)</span>'\n",
    "news = re.findall(p_news, info, re.S)\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_date = '<span class=\"c-color-gray2 c-font-normal\">(.*?)</span>'\n",
    "date = re.findall(p_date, info, re.S)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('C:\\\\Users\\\\16408\\\\python_jupyter\\\\爬虫\\\\测试.txt', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write('把内容输入到txt中，就是这么简单')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('测试.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、批量获取多家新闻信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu(company):\n",
    "    url = 'https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&wd=' + company\n",
    "    res = requests.get(url, headers = headers).text\n",
    "    \n",
    "    file1 = open('C:\\\\Users\\\\16408\\\\python_jupyter\\\\爬虫\\\\挖掘.txt', 'a')\n",
    "    file1.write(company + '数据挖掘completed!' + '\\n' + '\\n')\n",
    "    \n",
    "    for i in range(len(title)):\n",
    "        file1.write(str(i+1) + '.' + title[i] + '(' + date[i] + '-' + source[i] + ')' + '\\n')\n",
    "        file1.write(href[i] + '\\n')\n",
    "        file1.write('------------------------')\n",
    "        file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    companys = ['京东', '万科', '阿里巴巴', '腾讯']\n",
    "    for i in companys:\n",
    "        try:\n",
    "            baidu(i)\n",
    "            print(i + '新闻爬取成功')\n",
    "        except:\n",
    "            print(i + '新闻爬取失败')\n",
    "    time.sleep(10800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、一次性爬取多页信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu(page):\n",
    "    num = (page - 1)*10\n",
    "    url = ''https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&wd=阿里巴巴&pn='+str(num)\n",
    "    res = requests.get(url, headers = headers).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    baidu(i+1)\n",
    "    print('第' + str(i+1) + '页爬取成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8、批量爬取多家公司多页新闻信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu(company, page):\n",
    "    num = (page - 1)*10\n",
    "    url = ''https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&wd=' + company + '&pn=' + str(num)\n",
    "    res = requests.get(url, headers = headers, timeout = 10).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    companys = ['京东', '万科', '阿里巴巴', '腾讯']\n",
    "    for company in companys:\n",
    "        for i in range(20):\n",
    "            try:\n",
    "                baidu(company,i+1)\n",
    "                print(company + '第' + str(i+1) + '页新闻爬取成功')\n",
    "            except:\n",
    "                print(company + '第' + str(i+1) + '页新闻爬取失败')\n",
    "    time.sleep(10800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9、搜狗新闻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "url = 'https://www.sogou.com/sogou?query=%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4&interation=1728053249&pid=sogou-wsse-9fc36fa768a74fa9&ie=utf8&w=&sut=3402&sst0=1626249695000&lkt=2%2C1626249691480%2C1626249694897'\n",
    "res = requests.get(url, headers = headers).text\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_info = '<h3 class=\"vr-title\">(.*?)</h3>'\n",
    "info = re.findall(p_info, res, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_href = 'cacheStrategy=\"qcr:-1\" href=\"(.*?)\">'\n",
    "href = re.findall(p_href, res, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_title = '<em><!--red_beg-->(.*?)</a>'\n",
    "title = re.findall(p_title, res, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10、新浪财经"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "url = 'https://search.sina.com.cn/?q=阿里巴巴&c=news&from=channel&page=1&ie=utf-8'\n",
    "res = requests.get(url, headers = headers, timeout = 10).text\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "# url = 'https://search.sina.com.cn/?q=阿里巴巴&c=news&from=channel&page=2&ie=utf-8'\n",
    "# res = requests.get(url, headers = headers, timeout = 10).text\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://search.sina.com.cn/?q=阿里巴巴&c=news&from=channel&page=2&ie=utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'\n",
    "href = re.findall(p_href, res, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "title = re.findall(p_title, res, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "date = re.findall(p_date, res, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<.*?>', '', title[i])\n",
    "    date[i] = date[i].split(' ')[1]\n",
    "    print(str(i+1)+'.'+title[i]+'-'+date[i])\n",
    "    print(href[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlang(company, page):\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=' + str(page) + '&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers, timeout = 10).text  # 获取网页源代码\n",
    "    \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    \n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    \n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "    \n",
    "    for i in range(len(title)):                      # 数据清洗\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        print(str(i+1)+'.'+title[i]+'-'+date[i])\n",
    "        print(href[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xinlang('阿里巴巴', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companys = ['阿里巴巴', '腾讯', '新浪']\n",
    "for company in companys:\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            xinlang(company, i+1)\n",
    "            print(company + '第' + str(i+1) + '页新闻爬取成功')\n",
    "        except:\n",
    "            print(company + '第' + str(i+1) + '页新闻爬取失败')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1存入数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlang(company):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "    source = '新浪新闻'\n",
    "    other = '测试'\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=1&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers).text\n",
    "           \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    \n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    \n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "           \n",
    "    for i in range(len(title)):                      # 数据清洗\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        print('-----成功！-----')\n",
    "        \n",
    "    for i in range(len(title)):  # 存入数据库\n",
    "        db = pymysql.connect(host = 'localhost',port = 3306,user = 'root',password = '',database = 'pachong',charset = 'utf8')\n",
    "        cur = db.cursor()\n",
    "        sql = '''\n",
    "             INSERT INTO xinwen(company, title, href, date)\n",
    "             VALUES(%s, %s, %s, %s);\n",
    "             '''\n",
    "        cur.execute(sql, (company, title[i], href[i], date[i]))\n",
    "        db.commit()\n",
    "        cur.close()\n",
    "        db.close()\n",
    "        print('-----撒花！-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companys = ['阿里巴巴', '腾讯', '百度', '网易', '新浪']   # 多家公司爬取\n",
    "for company in companys:\n",
    "    try:\n",
    "        xinlang(company)\n",
    "        print(company+'爬取成功！')\n",
    "        pass\n",
    "    except:\n",
    "        print(company+'爬取失败！')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2数据去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    db = pymysql.connect(host = 'localhost',port = 3306,user = 'root',password = '',database = 'pachong',charset = 'utf8')\n",
    "    cur = db.cursor()\n",
    "    \n",
    "    sql_1 = '''\n",
    "    SELECT * FROM test WHERE company = '%s';\n",
    "    '''\n",
    "    cur.execute(sql_1, company)\n",
    "    data_all = cur.fetchall()\n",
    "    \n",
    "    title_all = []\n",
    "    for j in range(len(data_all)):\n",
    "        title_all.append(data_all[j][1])\n",
    "        \n",
    "    if title[i] not in title_all:\n",
    "        sql_2 = '''\n",
    "            INSERT INTO test(company, title, href, date, source, other)\n",
    "            VALUES(%s, %s, %s, %s, %s, %s);\n",
    "            '''\n",
    "        cur.execute(sql_2, (company, title[i], href[i], date[i], source, other))\n",
    "        db.commit()\n",
    "        cur.close()\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3数据清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(len(title)):\n",
    "    date[i] = date[i].split(' ')[0]\n",
    "    date[i] = re.sub('年', '-', date[i])\n",
    "    date[i] = re.sub('月', '-', date[i])\n",
    "    date[i] = re.sub('日', '', date[i])\n",
    "    if ('小时' in date[i]) or ('分钟' in date[i]):\n",
    "        date[i] = time.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        date[i] = date[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.4文本内容深度过滤——剔除噪声数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里介绍两种方法：第一种是根据新闻标题进行简单过滤，第二种则是根据新闻正文内容进行深度过滤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.4.1 根据新闻标题进行简单过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先遍历标题列表，如果标题不包含公司名称，则把相应的标题、网址、日期、来源都赋值为空值，然后批量删除列表中的空元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    if company not in title[i]:\n",
    "        title[i] = ''\n",
    "        href[i] = ''\n",
    "        date[i] = ''\n",
    "        source[i] = ''\n",
    "        other[i] = ''\n",
    "while '' in title:\n",
    "    title.remove('')\n",
    "while '' in href:\n",
    "    href.remove('')\n",
    "while '' in date:\n",
    "    date.remove('')\n",
    "while '' in source:\n",
    "    source.remove('')\n",
    "while '' in other:\n",
    "    other.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.4.2 根据新闻正文内容进行深度过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之前已经获取到了每一条新闻的网址，那么只要补写如下一行代码就能轻松地进行正文爬取。这行代码其实就是通过Requests库来访问每一条新闻的网址，并获取相应的网页源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    try:\n",
    "        article = requests.get(href[i], headers = headers, re.S).text\n",
    "    except:\n",
    "        print('单个新闻爬取失败！')\n",
    "    if company not in article:\n",
    "        title[i] = ''\n",
    "        href[i] = ''\n",
    "        date[i] = ''\n",
    "        source[i] = ''\n",
    "        other[i] = ''\n",
    "    while '' in title:\n",
    "        title.remove('')\n",
    "    while '' in href:\n",
    "        href.remove('')\n",
    "    while '' in date:\n",
    "        date.remove('')\n",
    "    while '' in source:\n",
    "        source.remove('')\n",
    "    while '' in other:\n",
    "        other.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑同一家公司名称简写与非简写：如华能信托”和“华能贵诚信托”指的是同一家公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_re = company[0] + '.{0,5}' + company[-1]\n",
    "if len(re.findall(company_re, article)) < 1:  # 如果没找到相关内容，则列表长度小于1，执行将该新闻赋值为空值然后清除的操作。\n",
    "    title[i] = ''\n",
    "    href[i] = ''\n",
    "    date[i] = ''\n",
    "    source[i] = ''\n",
    "    other[i] = ''\n",
    "while '' in title:\n",
    "    title.remove('')\n",
    "while '' in href:\n",
    "    href.remove('')\n",
    "while '' in date:\n",
    "    date.remove('')\n",
    "while '' in source:\n",
    "    source.remove('')\n",
    "while '' in other:\n",
    "    other.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.5 数据乱码处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导致这种情况的主要原因是Python获得的网页源代码的编码方式和网页实际的编码方式不一致，从而导致其中的中文呈现为乱码。此时需要对编码进行分析，并重新编码和解码，以获取想要的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.baidu.com'\n",
    "res = requests.get(url).text\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先通过requests.get(url).encoding查看Python获得的网页源代码的编码方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = requests.get(url).encoding\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页实际编码方式的查看方法为：在浏览器中打开网页后按F12键，打开开发者工具，如下图所示，展开最上方的<head>标签（<head>标签主要用来存储编码方式、网站标题等信息），其中<meta>标签里的charset参数存储的便是网页实际的编码方式，可以看到，网页实际的编码方式为utf-8，这与Python获取到的ISO-8859-1编码方式不一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utf-8、ISO-8859-1都是文本的编码方式，其中utf-8支持中文编码，而ISO-8859-1属于单字节编码，应用于英文系列，无法表示中文字符，这也是Python获取的内容出现中文乱码的原因，此时就需要通过重新编码及解码来解决该问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.5.1重新编码及解码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码是指把文本字符串转换为二进制字符（由英文字母和数字组成的原始字符），解码则是指把二进制字符转换为文本字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url).text\n",
    "res = res.encode('ISO-8859-1').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了utf-8编码外，国内网页常见的编码方式还有gbk编码，该编码方式也是支持中文的。如果通过Python获取的网页源代码的编码方式为ISO-8859-1，而网页实际的编码方式为gbk，则可用如下代码进行处理，其实就是把前面代码中的utf-8换成gbk。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = requests.get(url).text\n",
    "# res = res.encode('ISO-8859-1').decode('gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode()函数的功能是把字符串转换为原始的二进制字符\n",
    "'百度'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode()函数将由英文字母和数字组成的二进制字符转换为了一个中文字符串\n",
    "b'\\xe7\\x99\\xbe\\xe5\\xba\\xa6'.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11、舆情数据评分系统搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1根据标题评分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的评分方式就是根据新闻标题中是否出现特定的负面词来给新闻打分，主要用到的是in逻辑运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "keywords = ['违约', '诉讼', '兑付', '风险']\n",
    "for i in range(len(title)):\n",
    "    num = 0\n",
    "    for k in keywords:\n",
    "        if k in title[i]:\n",
    "            num -= 5\n",
    "            score.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlang1(company):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "    source = '新浪新闻'\n",
    "    other = '测试'\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=1&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers).text\n",
    "           \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    \n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    \n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "     \n",
    "    score = []\n",
    "    keywords = ['违约', '诉讼', '兑付', '风险', '破产', '监管']\n",
    "    for i in range(len(title)):\n",
    "        num = 0\n",
    "        for k in keywords:\n",
    "            if k in title[i]:\n",
    "                num -= 5\n",
    "            score.append(num)\n",
    "            \n",
    "    for i in range(len(title)):                      # 数据清洗\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        print(company+'这条舆情评分为'+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinlang1('滴滴')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2根据正文内容评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    article = requests.get(href[i], headers = headers).text\n",
    "except:\n",
    "    print('单个新闻爬取失败')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlang2(company):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "    source = '新浪新闻'\n",
    "    other = '测试'\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=1&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers).text\n",
    "           \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    \n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    \n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "     \n",
    "    score = []\n",
    "    keywords = ['违约', '诉讼', '兑付', '风险', '破产', '监管']\n",
    "    for i in range(len(title)):\n",
    "        num = 0\n",
    "        try:\n",
    "            article = requests.get(href[i], headers = headers).text\n",
    "        except:\n",
    "            print('单个新闻爬取失败')\n",
    "        for k in keywords:\n",
    "            if (k in title[i]) or (k in article):\n",
    "                num -= 5\n",
    "            score.append(num)\n",
    "            \n",
    "    for i in range(len(title)):                      # 数据清洗\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        print(company+'这条舆情评分为'+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinlang2('美团')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3解决乱码问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一条新闻的网址，我们并不清楚它所对应网页的实际编码方式，所以在爬取时很容易出现乱码，导致爬取到的内容是无效的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    res = res.encode('ISO-8859-1').decode('utf-8')\n",
    "except:\n",
    "    try:\n",
    "        res = res.encode('ISO-8859-1').decode('gbk')\n",
    "    except:\n",
    "        res = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlang3(company):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "    source = '新浪新闻'\n",
    "    other = '测试'\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=1&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers).text\n",
    "           \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    \n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    \n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "     \n",
    "    score = []\n",
    "    keywords = ['违约', '诉讼', '兑付', '风险', '破产', '监管']\n",
    "    for i in range(len(title)):\n",
    "        num = 0\n",
    "        try:\n",
    "            article = requests.get(href[i], headers = headers).text\n",
    "        except:\n",
    "            print('单个新闻爬取失败')\n",
    "            \n",
    "        try:\n",
    "            article = article.encode('ISO-8859-1').decode('utf-8')\n",
    "        except:\n",
    "            try:\n",
    "                article = article.encode('ISO-8859-1').decode('gbk')\n",
    "            except:\n",
    "                article = article\n",
    "                \n",
    "        for k in keywords:\n",
    "            if (k in title[i]) or (k in article):\n",
    "                num -= 5\n",
    "            score.append(num)\n",
    "            \n",
    "    for i in range(len(title)):                      # 数据清洗\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        print(company+'这条舆情评分为'+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinlang3('饿了么')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4处理非相关信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "段落内容通常由`<p></p>`包围，而大部分网页的正文内容都包含在`<p></p>`;\n",
    "\n",
    "“热门排行”内容在网页源代码中的结构，发现它不是由`<p></p>`标签包围，而是由`<li></li>`标签包围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_article = '<p>(.*?)</p>'\n",
    "article_main = re.findall(p_article, article)\n",
    "article = ''.join(article_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlang4(company):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "    source = '新浪新闻'\n",
    "    other = '测试'\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=1&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers).text\n",
    "           \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    \n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    \n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "     \n",
    "    score = []\n",
    "    keywords = ['违约', '诉讼', '兑付', '风险', '破产', '监管']\n",
    "    for i in range(len(title)):\n",
    "        num = 0\n",
    "        try:\n",
    "            article = requests.get(href[i], headers = headers).text\n",
    "        except:\n",
    "            print('单个新闻爬取失败')\n",
    "            \n",
    "        try:\n",
    "            article = article.encode('ISO-8859-1').decode('utf-8')\n",
    "        except:\n",
    "            try:\n",
    "                article = article.encode('ISO-8859-1').decode('gbk')\n",
    "            except:\n",
    "                article = article\n",
    "                \n",
    "        p_article = '<p>(.*?)</p>'\n",
    "        article_main = re.findall(p_article, article)\n",
    "        article = ''.join(article_main) \n",
    "        \n",
    "        for k in keywords:\n",
    "            if (k in title[i]) or (k in article):\n",
    "                num -= 5\n",
    "            score.append(num)\n",
    "            \n",
    "    for i in range(len(title)):                      # 数据清洗\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        print(company+'这条舆情评分为'+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinlang4('顺丰')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12、完整的新浪财经新闻数据挖掘系统搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1将舆情数据评分存入数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/查询数据，为去重做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_1 = 'SELECT * FROM article WHERE company = %s;'\n",
    "cur.execute(sql_1, company)\n",
    "data_all = cur.fetchall()\n",
    "title_all = []\n",
    "for j in range(len(title)):\n",
    "    title_all.append(data_all[j][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/判断爬取到的数据是否在数据库，不在的话存入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之所以用%s来占位，是因为cur.execute()在执行SQL语句时会自动把里面的内容都转换为字符串类型，而这时如果用%d来占位，就会出现类型不匹配的错误提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if title[i] not in title_all:\n",
    "    sql_2 = 'INSERT INTO article(company, title, href, date, source) VALUES(%s,%s,%s,%s,%s);'\n",
    "    cur.execute(sql_2, (company, title[i], href[i], date[i], source[i]))\n",
    "    db.commit()\n",
    "    cur.close()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.2完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xinlangcaijing(company, page):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.67'}\n",
    "    url = 'https://search.sina.com.cn/?q=' + company + '&c=news&from=channel&page=' + str(page) + '&ie=utf-8'\n",
    "    res = requests.get(url, headers = headers).text\n",
    "    \n",
    "    p_href = '<h2><a href=\"(.*?)\" target=\"_blank\">'   # 利用正则提取想要的内容\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    p_title = '<h2><a href=\".*?\" target=\"_blank\">(.*?)</a>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    p_date = '<span class=\"fgray_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "    \n",
    "    for i in range(len(title)):  # 数据处理\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[1]\n",
    "        date[i] = re.sub('年', '-', date[i])\n",
    "        date[i] = re.sub('月', '-', date[i])\n",
    "        date[i] = re.sub('日', '', date[i])\n",
    "        if ('小时' in date[i]) or ('分钟' in date[i]):\n",
    "            date[i] = time.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            date[i] = date[i]\n",
    "        \n",
    "    score = []   # 单个新闻爬取\n",
    "    keywords = ['违约', '诉讼', '兑付', '风险', '破产', '监管']\n",
    "    for i in range(len(title)):\n",
    "        num = 0\n",
    "        try:\n",
    "            article = requests.get(href[i], headers = headers).text\n",
    "        except:\n",
    "            print('单个新闻爬取失败')\n",
    "            \n",
    "        try:\n",
    "            article = article.encode('ISO-8859-1').decode('utf-8')\n",
    "        except:\n",
    "            try:\n",
    "                article = article.encode('ISO-8859-1').decode('gbk')\n",
    "            except:\n",
    "                article = article\n",
    "                \n",
    "        p_article = '<p>(.*?)</p>'    # 打分\n",
    "        article_main = re.findall(p_article, article)\n",
    "        article = ''.join(article_main) \n",
    "        \n",
    "        for k in keywords:\n",
    "            if (k in title[i]) or (k in article):\n",
    "                num -= 5\n",
    "            score.append(num)\n",
    "            \n",
    "    company_re = company[0] + '.{0,5}' + company[-1]  # 公司名称简称与全称\n",
    "    if len(re.findall(company_re, article)) < 1:  # 如果没找到相关内容，则列表长度小于1，执行将该新闻赋值为空值然后清除的操作。\n",
    "        title[i] = ''\n",
    "        href[i] = ''\n",
    "        date[i] = ''\n",
    "        score[i] = ''\n",
    "    while '' in title:  # 删除重复数据\n",
    "        title.remove('')\n",
    "    while '' in href:\n",
    "        href.remove('')\n",
    "    while '' in date:\n",
    "        date.remove('')\n",
    "    while '' in score:\n",
    "        score.remove('')\n",
    "        \n",
    "    for i in range(len(title)):  # 打印信息\n",
    "        print(str(i+1)+title[i]+'('+date[i]+')')\n",
    "        print(href[i])\n",
    "        print(company+'该条新闻舆情得分为'+str(score[i]))\n",
    "        \n",
    "    for i in range(len(title)):  # 数据存入数据库\n",
    "        db = pymysql.connect(host = 'localhost',port = 3306,user = 'root',password = '',database = 'pachong',charset = 'utf8')\n",
    "        cur = db.cursor()\n",
    "        sql_1 = '''\n",
    "             SELECT * FROM  test WHERE company = %s;\n",
    "             '''\n",
    "        cur.execute(sql_1, company)\n",
    "        data_all = cur.fetchall()\n",
    "        title_all = []\n",
    "        for j in range(len(data_all)):\n",
    "            title_all.append(data_all[j][1])\n",
    "            \n",
    "        if title[i] not in title_all:\n",
    "            sql_2 = 'INSERT INTO article(company, title, href, date, score) VALUES(%s,%s,%s,%s,%s);'\n",
    "            cur.execute(sql_2, (company, title[i], href[i], date[i], score[i]))\n",
    "            db.commit()\n",
    "            cur.close()\n",
    "            db.close()\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xinlangcaijing('阿里巴巴')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3多家公司多页爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companys = ['美团', '腾讯', '网易', '京东', '阿里巴巴']\n",
    "for company in companys:\n",
    "    for i in range(1, 20):\n",
    "        try:\n",
    "            xinlangcaijing(company, i+1)\n",
    "            print(company+'第'+str(i+1)+'页数据爬取存入成功！')\n",
    "        except:\n",
    "            print(company+'第'+str(i+1)+'页数据爬取存入失败！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13、分析与股价关系(接口失效)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1 获取得分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymysql.connect(host = 'localhost',port = 3306,user = 'root',password = '',database = 'pachong',charset = 'utf8')\n",
    "company = '腾讯'   # 连接\n",
    "\n",
    "date_list = list(pd.date_range('2021-07-01','2021-7-31'))  # 选定日期区间\n",
    "for i in range(len(date_list)):\n",
    "    date_list[i] = datetime.datetime.strftime(date_list[i], '%Y-%m-%d')\n",
    "    \n",
    "cur = db.cursor()    # 查询数据\n",
    "sql = 'SELECT * FROM article WHERE company = %s AND date = %s;' \n",
    "\n",
    "score_list = {}\n",
    "for d in date_list:\n",
    "    cur.execute(sql, (company, d))\n",
    "    data = cur.fetchall()\n",
    "    score = 100\n",
    "    for i in range(len(data)):\n",
    "        score += data[i][-1]\n",
    "    score_list[d] = score\n",
    "db.commit()\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(score_list, orient='index', columns = ['score'])\n",
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('score.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.2获取股价数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tushare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts.get_hist_data('00700', start = '2021-07-01', end = '2021-07-30')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = ts.pro_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14、爬虫技术进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14.1 IP代理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IP代理就是IP地址伪装，把本机的IP地址伪装成其他IP地址。IP代理服务提供商一般拥有海量IP地址，这些海量IP地址被称为IP代理池。我们购买IP代理服务后，所要做的就是在IP代理池里提取IP地址，然后写到自己的Python程序里，这样就可以把自己的IP地址伪装成其他IP地址，从而躲过某些网站对于固定IP地址访问次数的限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "proxy = requests.get('API链接').text  # 购买\n",
    "proxy = proxy.strip()\n",
    "proxies = {\"http\":\"http://\"+proxy, \"https\":\"https://\"+proxy}\n",
    "url = 'https://httpbin.org/get'\n",
    "res = requests.get(url, headers = headers, timeout = 10, proxies = proxies).text\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14.2Selenium库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium库是一个自动化测试工具，能够驱动浏览器模拟人的操作，如鼠标单击、键盘输入等。通过Selenium库能够比较容易地获取到网页的源代码，还可以进行网络内容的批量自动下载等。\n",
    "\n",
    "本小节将解决爬虫技术的一大难题：获取网页“真正”的源代码，或者说数据挖掘需要的源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过F12键可以看到的内容，为什么用Python却爬取不到呢？这是因为通过F12键看到的其实是网站动态渲染后的内容，它能够被常规手段爬取的信息很有限。一个快速验证的办法就是在网页上右击，然后选择“查看网页源代码”命令，所看到的网页源代码内容很少，也没有显示通过F12键能看到的信息，如下图所示。此时就可以判定通过F12键看到的网页源代码是动态渲染后的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "面对这种动态渲染的网页，在数据挖掘时就需要使用Selenium库，通过模拟人在浏览器中的操作，快速获取渲染后的网页源代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果官网访问不了，可以在百度上搜索“ChromeDriver下载”，会找到如下图所示的镜像网站，网址为：http://npm.taobao.org/mirrors/chromedriver/。使用上面的方法在这个镜像网站中也能下载ChromeDriver。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChromeDriver安装下载完ChromeDriver之后，还需要把它安装到Python的安装路径中，让Python能更方便地调用它。下面以Windows系统为例讲解具体方法。第一步：按快捷键Win+R调出“运行”对话框，输入“cmd”，按Enter键。第二步：在弹出的命令行窗口中输入“where python”，按Enter键，会显示Python的安装路径。第三步：打开Windows文件资源管理器，进入Python的安装路径，找到并打开Scripts文件夹。第四步：将下载好的chromedriver_win32.zip解压，将得到的exe文件复制到Scripts文件夹中。第五步：再次打开命令行窗口，输入“chromedriver”，按Enter键，如果显示类似下图所示的信息，就说明ChromeDriver安装成功了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://www.baidu.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath是一个定位各个元素的手段，可以把XPath理解为这个元素的名字或id。\n",
    "\n",
    "按F12键打开开发者工具，利用“选择”按钮选中搜索框，然后在搜索框对应的那一行源代码上右击，在弹出的快捷菜单中选择“Copy>Copy XPath”命令，把复制的内容（搜索框的XPath内容是“//*[@id=\"kw\"]”）粘贴到上面的代码里替换“XPath内容”即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('headless')\n",
    "\n",
    "browser = webdriver.Chrome(options = chrome_options)\n",
    "browser.get(\"https://www.baidu.com/\")\n",
    "\n",
    "browser.find_element_by_xpath('//*[@id=\"s_kw_wrap\"]').send_keys('python')\n",
    "browser.find_element_by_xpath('//*[@id=\"su\"]').clik()\n",
    "\n",
    "time.sleep(3)\n",
    "data = browser.page_source\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*[@id=\"s_kw_wrap\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "css_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_element_by_css_selector('#s_kw_wrap').send_keys('python')\n",
    "browser.find_element_by_css_selector('#su').clik()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = browser.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与Requests库相比，Selenium库的优势明显，不需要设置headers等参数就能非常方便地获取到一些Requests库难以获取的网页源代码，而且能模拟键盘和鼠标操作，写法也很简洁。有读者可能会产生这样的疑问：既然Selenium库这么厉害，为什么不对所有网站都用Selenium库来爬取呢？这是因为，通过Selenium库访问网站是相当于模拟人打开一个浏览器进行访问的，其访问速度要比Requests库慢很多，所以，对于普通的网站，使用Requests库来爬取，对于Requests库无法获取网页源代码的复杂网站，再使用Selenium库来爬取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15、新浪财经股票实时数据挖掘实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brower = webdriver.Chrome()\n",
    "brower.get('http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml')\n",
    "data = brower.page_source\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无界面浏览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()    # 设置无界面浏览\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "brower = webdriver.Chrome(options = chrome_options)     # 浏览网站\n",
    "brower.get('http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml')\n",
    "\n",
    "data = brower.page_source   # 获取页面数据\n",
    "# print(data)\n",
    "\n",
    "brower.quit()\n",
    "\n",
    "p_price = '<div id=\"price\" class=\".*?\">(.*?)</div>'    # 获取上证价格指数\n",
    "price = re.findall(p_price, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16、东方财富网数据挖掘实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "东方财富网（http://www.eastmoney.com/） 是一家专业的互联网财经媒体，提供7×24小时财经资讯及全球金融市场报价，汇聚全方位的综合财经新闻和金融市场资讯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16.1单个公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "brower = webdriver.Chrome(options = chrome_options)\n",
    "brower.get('https://so.eastmoney.com/news/s?keyword=阿里巴巴')\n",
    "\n",
    "df = brower.page_source\n",
    "# print(df)\n",
    "brower.quit()\n",
    "\n",
    "p_title = '<div class=\"news_item_t\"><a href=\".*?\" target=\"_blank\">(.*?)</a></div>'\n",
    "title = re.findall(p_title, df, re.S)\n",
    "\n",
    "p_href = '<div class=\"news_item_t\"><a href=\"(.*?)\" target=\"_blank\">.*?</a></div>'\n",
    "href = re.findall(p_href, df, re.S)\n",
    "\n",
    "p_date = '<span class=\"news_item_time\">(.*?)</span>'\n",
    "date = re.findall(p_date, df, re.S)\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<.*?>', '', title[i])\n",
    "    date[i] = date[i].split(' ')[0]\n",
    "    print(str(i+1)+'.'+title[i]+'-'+date[i])\n",
    "    print(href[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16.2多家公司爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import pymysql\n",
    "\n",
    "def dongfang(company):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    \n",
    "    browser = webdriver.Chrome(options = chrome_options)\n",
    "    url = 'https://so.eastmoney.com/news/s?keyword=' + company\n",
    "    \n",
    "    browser.get(url)\n",
    "    res = browser.page_source\n",
    "    browser.quit()\n",
    "    \n",
    "    p_title = '<div class=\"news_item_t\"><a href=\".*?\" target=\"_blank\">(.*?)</a></div>'\n",
    "    title = re.findall(p_title, res, re.S)\n",
    "    p_href = '<div class=\"news_item_t\"><a href=\"(.*?)\" target=\"_blank\">.*?</a></div>'\n",
    "    href = re.findall(p_href, res, re.S)\n",
    "    p_date = '<span class=\"news_item_time\">(.*?)</span>'\n",
    "    date = re.findall(p_date, res, re.S)\n",
    "    \n",
    "    for i in range(len(title)):\n",
    "        title[i] = re.sub('<.*?>', '', title[i])\n",
    "        date[i] = date[i].split(' ')[0]\n",
    "        \n",
    "    for i in range(len(title)):\n",
    "        db = pymysql.connect(host = 'localhost',port = 3306,user = 'root',password = '',database = 'pachong',charset = 'utf8')\n",
    "        cur = db.cursor()\n",
    "        sql_1 = 'SELECT * FROM dongfangcaifu WHERE company=%s;'\n",
    "        cur.execute(sql_1,company)\n",
    "        \n",
    "        data_all = cur.fetchall()\n",
    "        title_all = []\n",
    "        for j in range(len(data_all)):\n",
    "            title_all.append(data_all[j][1])\n",
    "            \n",
    "        if title[i] not in title_all:\n",
    "            sql_2 = 'INSERT INTO dongfangcaifu(company,title,href,date) VALUES(%s,%s,%s,%s);'\n",
    "            cur.execute(sql_2, (company, title[i], href[i], date[i]))\n",
    "            db.commit()\n",
    "            cur.close()\n",
    "            db.close()\n",
    "            \n",
    "        print('----'+company+'存储成功----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dongfang('阿里巴巴')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companys = ['阿里巴巴', '腾讯',  '京东', '网易']\n",
    "for company in companys:\n",
    "    try:\n",
    "        dongfang(company)\n",
    "        print(company+'数据存储成功！')\n",
    "    except:\n",
    "        print(compang+'数据存储失败！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17、裁判文书网数据挖掘实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "裁判文书网（http://wenshu.court.gov.cn/） 是最权威的生效裁判文书公示网站，对于金融行业内部风控和舆情监控有较高的参考价值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('http://wenshu.court.gov.cn')\n",
    "browser.maximize_window()\n",
    "browser.find_element_by_xpath('//*[@id=\"_view_1540966814000\"]/div/div/div[2]/input').clear()\n",
    "browser.find_element_by_xpath('//*[@id=\"_view_1540966814000\"]/div/div/div[2]/input').send_keys('房地产')\n",
    "browser.find_element_by_xpath('//*[@id=\"_view_1540966814000\"]/div/div/div[3]').click()\n",
    "time.sleep(10)\n",
    "data = browser.page_source\n",
    "browser.quit()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18、巨潮资讯网数据挖掘实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "巨潮资讯网（http://www.cninfo.com.cn/new/index） 是中国证券监督管理委员会指定的上市公司信息披露网站，创建于1995年，是国内最早的证券信息专业网站，也是国内首家全面披露深沪3000多家上市公司公告信息和市场数据的大型证券专业网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "brower = webdriver.Chrome()\n",
    "url = 'http://www.cninfo.com.cn/new/fulltextSearch?notautosubmit=&keyWord=理财'\n",
    "brower.get(url)\n",
    "data = browser.page_source\n",
    "print(data)  # 计算机拒绝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19、通过PDF文本解析上市公司理财公告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据挖掘技术批量下载巨潮资讯网的上市公司理财公告PDF文件，并通过PDF文本解析技术分析获取到的理财公告PDF文件，从中识别潜在的机构投资者，即合适的资金方。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前爬取多页用的是修改网址参数的方法，但这种方法对巨潮资讯网不适用，因为在巨潮资讯网进行翻页操作时，网址并没有发生变化。此时可以利用Selenium库模拟鼠标单击下图所示的“下一页”按钮，并根据公告数量来确定模拟单击的次数，每单击一次就获取一下该页的源代码，最后把获取到的各页的源代码存储到一个列表里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在获取公告总数时，一定要记得在findall()最后写一个[0]，因为findall()获取到的是一个列表，尽管这个列表只有一个元素，仍然要写一个[0]来提取列表中的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.1爬取多页（网页不会变时）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "brower = webdriver.Chrome(options = chrome_options)\n",
    "url = 'http://www.cninfo.com.cn/new/fulltextSearch?notautosubmit=&keyWord=理财'\n",
    "brower.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "data = browser.page_source\n",
    "\n",
    "p_count = '</div><span class=\"total-box\" style>约(.*?) 条.*?</span>'\n",
    "count = re.findall(p_count, data)[0]\n",
    "pages = int(int(count)/10)\n",
    "\n",
    "datas = []\n",
    "datas.append(data)\n",
    "for i in range(pages):\n",
    "    browser.find_element_by_xpath('//*[@id=\"fulltext-search\"]/div/div[1]/div[2]/div[4]/div[2]/div/button[2]/i').click()\n",
    "    time.sleep(2)\n",
    "    data = browser.page_source\n",
    "    datas.append(data)\n",
    "    time.sleep(1)\n",
    "    \n",
    "all_data = ''.join(datas)\n",
    "browser.quit()\n",
    "\n",
    "p_title = ''\n",
    "p_href = ''\n",
    "p_date = ''\n",
    "title = re.findall(p_title, all_data)\n",
    "href = re.findall(p_href, all_data)\n",
    "date = re.findall(p_date, all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.2自动筛选所需内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    if ('2018' in date[i]) or ('2019' in date[i]):\n",
    "        title[i] = title[i]\n",
    "        href[i] = href[i]\n",
    "        date[i] = date[i]\n",
    "    else:\n",
    "        title[i] = ''\n",
    "        href[i] = ''\n",
    "        date[i] = ''\n",
    "while '' in title:\n",
    "    title.remove('')\n",
    "while '' in href:\n",
    "    href.remove('')\n",
    "while '' in date:\n",
    "    date.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    if ('刚兑' in title[i]) or ('保底' in title[i]):\n",
    "        title[i] = ''\n",
    "        href[i] = ''\n",
    "        date[i] = ''\n",
    "    else:\n",
    "        title[i] = title[i]\n",
    "        href[i] = href[i]\n",
    "        date[i] = date[i]\n",
    "while '' in title:\n",
    "    title.remove('')\n",
    "while '' in href:\n",
    "    href.remove('')\n",
    "while '' in date:\n",
    "    date.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.3理财公告PDF文件的自动批量下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "brower = webdriver.Chrome()\n",
    "url = 'http://www.cninfo.com.cn/new/disclosure/detail?plate=&orgId=9900014608&stockCode=002479&announcementId=1205697786&announcementTime=2019-01-02'\n",
    "brower.get(url)\n",
    "browser.find_element_by_xpath('/html/div/div[1]/div[2]/div[1]/div/a[4]').click()\n",
    "browser.quit()  # 计算机拒绝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(href)):  # 默认存储到c盘，可修改\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {'profile.default_content_settings.popups':0, 'download.default_directory':'d:\\\\公告'}\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "    browser = webdriver.Chrome(options = chrome_options)\n",
    "    brower.get(href[i])\n",
    "    try:\n",
    "        browser.find_element_by_xpath('/html/div/div[1]/div[2]/div[1]/div/a[4]').click()\n",
    "        time.sleep(3)\n",
    "        browser.quit() \n",
    "        print('下载成功！')\n",
    "    except:\n",
    "        print('下载失败！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.4 PDF文本解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python中有多个用于解析PDF文本的库，如pdfplumber库、pdfminer库、Tabula库等。经笔者测试， pdfplumber库是目前使用最方便的库，它不仅能解析文字，还能方便地解析表格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.4.1用pdfplumber库提取文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "pdf = pdfplumber.open('富春环保.PDF')  # 打开\n",
    "pages = pdf.pages    # 获取信息\n",
    "text_all = []\n",
    "for page in pages:\n",
    "    text = page.extract_text()\n",
    "    text_all.append(text)\n",
    "text_all = ''.join(text_all)\n",
    "print(text_all)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.4.2用pdfplumber库提取表格内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "pdf = pdfplumber.open('富春环保2021年第一季度报告.PDF')  # 打开\n",
    "pages = pdf.pages    # 获取信息\n",
    "page = pages[5]\n",
    "tables = page.extract_tables()\n",
    "table = tables[0]\n",
    "print(table) # 此时获得的table是一个嵌套列表结构，即大列表里包含多个小列表，每个小列表的内容即表格中每一行的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(table[1:],columns=table[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.5PDF文本解析实战——寻找合适的理财公告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.5.1遍历文件夹里所有的PDF文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中files[0]表示母文件夹信息，files[1]表示各个子文件夹信息，files[2]表示母文件夹和子文件夹里的各个文件信息。一般用files[2]来获取文件信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_dir = r'C:\\Users\\16408\\python_jupyter\\爬虫'\n",
    "file_list = []\n",
    "for files in os.walk(file_dir):\n",
    "    print(files[0])\n",
    "    print('----------')\n",
    "    print(files[1])\n",
    "    print('----------')\n",
    "    print(files[2])\n",
    "    for file in files[2]:\n",
    "        if os.path.splitext(file)[1] == '.PDF' or os.path.splitext(file)[1] == '.pdf':\n",
    "            file_list.append(file_dir + '\\\\' + file)\n",
    "            \n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.5.2批量解析每一个PDF文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_list)):\n",
    "    pdf = pdfplumber.open(file_list[i])\n",
    "    pages = pdf.pages\n",
    "    text_all = []\n",
    "    for page in pages:\n",
    "        text = page.extract_text()\n",
    "        text_all.append(text)\n",
    "text_all = ''.join(text_all)\n",
    "# print(text_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.5.3将合格的PDF文件自动归档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.rename(pdf_i, newpath)的第1个参数pdf_i是pdfs列表里的每一个旧的PDF文件路径，第2个参数newpath则是新创建的保存路径。该代码执行之后，就会把筛选出来的PDF文件移动到新的文件夹里了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = []\n",
    "for i in range(len(file_list)):\n",
    "    pdf = pdfplumber.open(file_list[i])\n",
    "    pages = pdf.pages\n",
    "    text = page.extract_text()\n",
    "    if '现金管理' in text:\n",
    "        pdfs.append(file_list[i])\n",
    "        \n",
    "for pdf_i in pdfs:  # 筛选后的文件集中到一个文件夹中\n",
    "    newpath = 'E:\\\\需要的文件夹' + pdf_i.split('\\\\')[-1]\n",
    "    os.rename(pdf_i, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20、邮件提醒系统搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的章节里，我们进行了很多数据的挖掘与分析工作，获取到分析结果后，还需将其发送给相关人员。本章便主要介绍如何通过Python搭建邮件提醒系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20.1 通过腾讯QQ邮箱发送邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实战中需要修改的就是发件人信息、收件人信息、邮箱的SMTP授权码，以及邮件的正文内容和邮件主题。这里重点讲一下如何获取SMTP授权码。它并不是QQ邮箱的登录密码，而是一个程序自动发送邮件所需的授权码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "user = '1640873245@qq.com' # 发件人邮箱\n",
    "pwd = 'rmpizuhscwpsbefa' # qq邮箱smtp授权码\n",
    "to = '13588804720@163.com'  # 收件人邮箱，多人用逗号分隔\n",
    "\n",
    "msg = MIMEText('登录QQ邮箱，单击顶部的“设置”链接，然后单击“账户”标签')  # 邮件正文内容\n",
    "msg['Subject'] = 'python自动化发送邮件验证'\n",
    "msg['From'] = user\n",
    "msg['To'] = to\n",
    "\n",
    "s = smtplib.SMTP_SSL('smtp.qq.com', 465)\n",
    "s.login(user, pwd)  # 登录\n",
    "s.send_message(msg)  # 发送\n",
    "s.quit()             # 退出\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：登录QQ邮箱，单击顶部的“设置”链接，然后单击“账户”标签；\n",
    "\n",
    "第二步：在“账户”选项卡中向下滚动，直到看到如下图所示的选项，单击“POP3/SMTP服务”右侧的“开启”链接；\n",
    "\n",
    "第三步：单击“开启”链接后，会有一个验证密保的过程。按照页面中的说明，向指定号码发送指定内容的手机短信，发送完毕后单击页面中的“我已发送”按钮，会弹出一个框，里面就包含SMTP授权码，把它复制并存储起来，方便以后调用。获得自己QQ邮箱的SMTP授权码后，便可以设置代码里的发件人、授权码及收件人，然后运行程序，感受用Python自动发送邮件成功的喜悦吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmpizuhscwpsbefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了腾讯QQ邮箱和网易163邮箱外，还可以根据自己的需要选择其他邮箱，只要更改该邮箱的SMTP授权码，并相应更改下面代码里的SMTP服务器地址和端口。所用邮箱的SMTP服务器地址和端口一般可在邮箱的帮助页面中查找。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = smtplib.SMTP_SSL('smtp服务器地址', 端口)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20.2发送HTML格式的邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面发送的邮件内容是文本格式的，实际上邮件还支持HTML格式的内容。HTML格式的邮件是指邮件里的内容就像网页内容一样，可以设置不同的格式，还可以包含链接，用于跳转到其他网页。在如下图所示的邮件正文中，显示为蓝色并带下画线的新闻标题都是链接，单击它们就能跳转到相应的新闻页面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要制作出链接效果，就需要用到2.2.3小节中的网页结构知识，构造一个段落和包含链接的文本内容"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p>段落内容</p>\n",
    "<a href=\"链接地址\">文本内容</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要实现HTML格式的邮件内容，只需要把原来的代码修改为如下内容："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIMEText括号里的参数中，第1个参数mail_msg指定邮件正文内容，第2个参数’html'表示邮件正文内容是按HTML格式显示的，第3个参数’utf-8’用来声明中文编码方式。在实战中需要注意的就是如何编写mail_msg中的邮件正文内容。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "因为这样会导致所有内容都在一行里，导致邮件正文不便于阅读。而在上面这行代码两边加上段落标签<p>与</p>，就能自动分段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_msg = '''\n",
    "<p>段落内容</p>\n",
    "<p><a href=\"链接地址\">文本内容</a></p>  \n",
    "'''\n",
    "# 两边加上<p></p>可以自动分段\n",
    "\n",
    "msg = MIMEText(mail_msg, html, 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20.3发送邮件附件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后便是添加附件的代码，如下所示。其中在写文件路径时，最好用两个反斜杠来防止单个反斜杠导致的特殊含义。这里的文件路径可以包含中文，因为设置了utf-8中文编码。如果提示文件路径有误，把里面的大写字母换成小写即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若想发送多个附件，只需把添加附件的代码重复几遍并修改相关内容。例如，把上面代码中的att1换成att2（att是attachment的缩写，意思为附件），然后修改文件路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个可以添加正文和附件的msg\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "msg = MIMEMultipart()\n",
    "\n",
    "msg.attach(MIMEText(mail_msg, html, 'utf-8'))  # 添加正文\n",
    "att1 = MIMEText(open('E:\\\\test.docx','rb').read(), 'base64', 'utf-8')\n",
    "att1['Content-Type'] = 'application/octet-stream'\n",
    "att1['Content-Disposition'] = 'attchment;filename=\"test.docx\"'\n",
    "msg.attch(att1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21、定时发送数据分析报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过Python调用数据库中的数据，通过邮件发送出去，并通过Schedule库实现每天定时发送，制作出一个每天定时发送上市公司数据分析报告邮件的邮件提醒系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21.1 用Python提取数据并发送数据分析报告邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接数据库\n",
    "db = pymysql.connect(host = 'localhost',\n",
    "                     port = 3306,\n",
    "                     user = 'root',\n",
    "                     password = '',\n",
    "                     database = 'pachong',\n",
    "                     charset = 'utf8')\n",
    "company = '阿里巴巴'\n",
    "date_u = '2021-07-14'\n",
    "cur = db.cursor()\n",
    "sql = 'SELECT * FROM article WHERE company = %s and date = %s;'\n",
    "cur.execute(sql, (company, date_u))\n",
    "data = cur.fetchall()\n",
    "db.commit()\n",
    "cur.close()\n",
    "db.close()\n",
    "# 这里获取到的data是一个元组（元组可以理解为一个不可修改的列表），里面有各个小元组存储着每条新闻的相关信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取完数据后就可以编写HTML格式的邮件正文内容了\n",
    "mail_msg = []\n",
    "for i in range(len(data)):\n",
    "    href = '<p><a href=\"' + data[i][2] + '\">' + data[i][1] + '</a></p>'\n",
    "    mail_msg.append(href)\n",
    "mail_msg = ''.join(mail_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "user = '1640873245@qq.com' # 发件人邮箱\n",
    "pwd = 'rmpizuhscwpsbefa' # qq邮箱smtp授权码\n",
    "to = '13588804720@163.com'  # 收件人邮箱，多人用逗号分隔\n",
    "\n",
    "# 连接数据库\n",
    "db = pymysql.connect(host = 'localhost',\n",
    "                     port = 3306,\n",
    "                     user = 'root',\n",
    "                     password = '',\n",
    "                     database = 'pachong',\n",
    "                     charset = 'utf8')\n",
    "company = '阿里巴巴'\n",
    "date_u = '2021-07-14'\n",
    "cur = db.cursor()\n",
    "sql = 'SELECT * FROM article WHERE company = %s and date = %s;'\n",
    "cur.execute(sql, (company, date_u))\n",
    "data = cur.fetchall()\n",
    "db.commit()\n",
    "cur.close()\n",
    "db.close()\n",
    "# 这里获取到的data是一个元组（元组可以理解为一个不可修改的列表），里面有各个小元组存储着每条新闻的相关信息。\n",
    "\n",
    "# 提取完数据后就可以编写HTML格式的邮件正文内容了\n",
    "# 邮件正文中只有带链接的新闻标题，显得不够正式。为邮件正文添加引言、内容标题及署名，并为带链接的新闻标题添加序号\n",
    "# 如果觉得邮件正文行距过大，可以在<p>标签里设置行距\n",
    "# 如果需要换行，可以添加表示换行的<br>标签\n",
    "mail_msg = []\n",
    "mail_msg.append('<p>尊敬的用户您好，以下是7-14的舆情监控，望查阅：</p>')\n",
    "mail_msg.append('<p><b>一、阿里巴巴：</b></p>')  # 标签加粗\n",
    "for i in range(len(data)):\n",
    "    href = '<p><a href=\"' + data[i][2] + '\">' + str(i+1) + '.' + data[i][1] + '</a></p>'\n",
    "    mail_msg.append(href)\n",
    "    mail_msg.append('<br>')\n",
    "    \n",
    "mail_msg.append('<p style=\"margin:0 auto\">祝好！</p>')\n",
    "mail_msg = ''.join(mail_msg)\n",
    "\n",
    "msg = MIMEText(mail_msg, 'html', 'utf-8')  # 邮件正文内容\n",
    "msg['Subject'] = '舆情监控'\n",
    "msg['From'] = user\n",
    "msg['To'] = to\n",
    "\n",
    "s = smtplib.SMTP_SSL('smtp.qq.com', 465)\n",
    "s.login(user, pwd)  # 登录\n",
    "s.send_message(msg)  # 发送\n",
    "s.quit()             # 退出\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21.2 用Python实现每天定时发送邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节将讲解在Python中实现定时任务的两种方法：第一种是利用while True循环配合time.sleep()函数来实现，其代码比较简单，但是效果比较粗糙，仅供学习；第二种则是利用Schedule库来实现，效果会更加精确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.2.1利用while True循环实现每天定时发送邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "user = '1640873245@qq.com' # 发件人邮箱\n",
    "pwd = 'rmpizuhscwpsbefa' # qq邮箱smtp授权码\n",
    "to = '13588804720@163.com'  # 收件人邮箱，多人用逗号分隔\n",
    "\n",
    "while True:\n",
    "# 连接数据库\n",
    "    db = pymysql.connect(host = 'localhost',\n",
    "                     port = 3306,\n",
    "                     user = 'root',\n",
    "                     password = '',\n",
    "                     database = 'pachong',\n",
    "                     charset = 'utf8')\n",
    "    company = '阿里巴巴'\n",
    "    date_u = '2021-07-14'\n",
    "    cur = db.cursor()\n",
    "    sql = 'SELECT * FROM article WHERE company = %s and date = %s;'\n",
    "    cur.execute(sql, (company, date_u))\n",
    "    data = cur.fetchall()\n",
    "    db.commit()\n",
    "    cur.close()\n",
    "    db.close()\n",
    "# 这里获取到的data是一个元组（元组可以理解为一个不可修改的列表），里面有各个小元组存储着每条新闻的相关信息。\n",
    "\n",
    "# 提取完数据后就可以编写HTML格式的邮件正文内容了\n",
    "# 邮件正文中只有带链接的新闻标题，显得不够正式。为邮件正文添加引言、内容标题及署名，并为带链接的新闻标题添加序号\n",
    "# 如果觉得邮件正文行距过大，可以在<p>标签里设置行距\n",
    "# 如果需要换行，可以添加表示换行的<br>标签\n",
    "    mail_msg = []\n",
    "    mail_msg.append('<p>尊敬的用户您好，以下是7-14的舆情监控，望查阅：</p>')\n",
    "    mail_msg.append('<p><b>一、阿里巴巴：</b></p>')  # 标签加粗\n",
    "    for i in range(len(data)):\n",
    "        href = '<p><a href=\"' + data[i][2] + '\">' + str(i+1) + '.' + data[i][1] + '</a></p>'\n",
    "        mail_msg.append(href)\n",
    "        mail_msg.append('<br>')\n",
    "    \n",
    "    mail_msg.append('<p style=\"margin:0 auto\">祝好！</p>')\n",
    "    mail_msg = ''.join(mail_msg)\n",
    "\n",
    "    msg = MIMEText(mail_msg, 'html', 'utf-8')  # 邮件正文内容\n",
    "    msg['Subject'] = '舆情监控'\n",
    "    msg['From'] = user\n",
    "    msg['To'] = to\n",
    "\n",
    "    s = smtplib.SMTP_SSL('smtp.qq.com', 465)\n",
    "    s.login(user, pwd)  # 登录\n",
    "    s.send_message(msg)  # 发送\n",
    "    s.quit()             # 退出\n",
    "    print('success!')\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.2.2利用Schedule库实现每天定时发送邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schedule.run_pending()的含义为运行所有可以运行的schedule任务，time.sleep(10)是让schedule任务运行完后休息10秒再检测是否有可以运行的任务，也可以直接写成time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def eating():\n",
    "    print('吃饭啦')\n",
    "\n",
    "schedule.every().day.at(\"12:00\").do(eating)\n",
    "# schedule.every(10).minutes.do(eating)\n",
    "\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22、表格数据的常规获取方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一种是文件形式的表格数据，如世界银行的项目表格数据；第二种是网页形式的表格数据，如新浪财经的大宗交易表格数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22.1文件形式的表格数据——世界银行项目表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://search.worldbank.org/api/projects/all.csv'\n",
    "res = requests.get(url)\n",
    "file = open('世界银行项目表.csv', 'wb')\n",
    "file.write(res.content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22.2 网页形式的表格数据——新浪财经大宗交易表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'http://vip.stock.finance.sina.com.cn/q/go.php/vInvestConsult/kind/dzjy/index.phtml'\n",
    "table = pd.read_html(url)[0]\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于和讯研报网来说，使用上一小节介绍的pd.read_html()函数解析表格会被网站拒绝访问，这时就需要利用第8章介绍的Selenium库来访问网页并获取网页源代码，然后使用pd.read_html()函数进行解析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "browser = webdriver.Chrome(options = chrome_options)\n",
    "url = 'http://yanbao.stock.hexun.com/xgq/gsyj.aspx'\n",
    "browser.get(url)\n",
    "data = browser.page_source\n",
    "# print(data)\n",
    "table = pd.read_html(data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Empty DataFrame\n",
       " Columns: [股票名称, 标题, 所属行业, 机构名称, 分析师, 评级分类, 评级变动, 上涨空间, 日期]\n",
       " Index: []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“股票代码”列的值是内容为一串数字的字符串，在读取时要利用dtype参数把该列的数据类型设置为str（字符串类型），否则pandas库会默认将它读取成数值类型，导致以0开头的股票代码在读取后丢失开头的0。例如，万科集团的股票代码“000002”会被读取成“2”，导致之后通过Tushare库查询股票代码失败。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel('分析师评级报告数据.xlsx', dtype = {'股票代码':str})\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna(thresh = 5)\n",
    "\n",
    "# 其含义是对每一行取“研究机构”和“分析师”这两列的值，并用“-”号将两个字符串连起来，组成一个新的列， axis=1则表示按行的方向进行数据选择。\n",
    "df['研究机构—分析师'] = df.apply(lambda x:x['研究机构']+'-'+x['分析师'], axis = 1)\n",
    "\n",
    "columns = ['股票代码', '机构名称', '研究机构—分析师', '最新评级', '评级调整', '报告日期']\n",
    "data = df[columns]\n",
    "\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "t = today - datetime.timedelta(days = 30)\n",
    "t = t.strftime('%Y-%m-%d')\n",
    "res = data[data['报告日期']<t]\n",
    "\n",
    "import tushare as ts\n",
    "# ts_result = ts.get_hist_data('000002', '2019-01-02', '2019-01-31')\n",
    "# start_price = ts_result.iloc[-1]['open']\n",
    "# end_price = ts_result.iloc[0]['close']\n",
    "# return_rate = (start_price / end_price) - 1.0\n",
    "\n",
    "df_use = data.iloc[0:100]\n",
    "rate = []\n",
    "for i, row in df_use.iterrows():\n",
    "    code = row['股票代码']\n",
    "    analysist_date = row['报告日期']\n",
    "    \n",
    "    begin_date = datetime.datetime.strptime(analysist_date, '%Y-%m-%d')#将原来字符串格式的日期转换成datetime.datetime格式的日期\n",
    "    begin_date = begin_date + datetime.timedelta(days=1)\n",
    "    begin_date = datetime.strftime('%Y-%m-%d')\n",
    "    end_date = datetime.datetime.strptime(analysist_date, '%Y-%m-%d')\n",
    "    end_date = end_date + datetime.timedelta(days=30)\n",
    "    end_date = datetime.strftime('%Y-%m-%d')\n",
    "    \n",
    "    ts_result = ts.get_hist_data(code, begin_date, end_date)\n",
    "    if ts_result is None or len(ts_result)<5:\n",
    "        return_rate = 0\n",
    "    else:\n",
    "        if ts_result.iloc[-1]['low']==ts_result.iloc[-1]['high'] and abs(ts_result.iloc[-1]['p_change']-10.0)<0.1:\n",
    "            return_rate = 0\n",
    "        else:\n",
    "            start_price = ts_result.iloc[-1]['open']\n",
    "            end_price = ts_result.iloc[0]['close']\n",
    "            return_rate = (start_price / end_price) - 1.0\n",
    "    rate.append(return_rate)\n",
    "    \n",
    "df_use['30天收益率'] = rate\n",
    "df_use.to_excel('30天收益率.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里介绍一下iterrows()函数，它用来遍历DataFrame的每一行，演示代码如下：\n",
    "\n",
    "i就是每一行的行索引序号，row就是每一行的内容，该内容是一个一维结构的Series对象，通过row[’股票代码’]及row[’报告日期’]就可以提取相应内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in df_use.iterrows():\n",
    "#     print(i)\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23、用Python创建Word文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting python-docx\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/8b/a0/52729ce4aa026f31b74cc877be1d11e4ddeaa361dc7aebec148171644b33/python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\16408\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from python-docx) (4.6.3)\n",
      "Using legacy 'setup.py install' for python-docx, since package 'wheel' is not installed.\n",
      "Installing collected packages: python-docx\n",
      "    Running setup.py install for python-docx: started\n",
      "    Running setup.py install for python-docx: finished with status 'done'\n",
      "Successfully installed python-docx-0.8.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\16408\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23.1用Python创建Word文档的基本知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word文档生成完毕！\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "file = docx.Document() #利用file=docx.Document()在后台创建一个Word文档对象，并赋值给file。\n",
    "file.add_paragraph('螃蟹在剥我的壳，笔记本在写我') #通过file.add_paragraph()函数添加段落\n",
    "file.add_paragraph('漫天的我落在枫叶上雪花上')\n",
    "file.add_paragraph('而你在想我')\n",
    "file.save('./三行情书.docx')\n",
    "print('word文档生成完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意用于保存Word文档的文件夹需要提前创建好。如果该Word文档已存在，则原有的Word文档会被新的Word文档替换，所以在执行程序时不要在其他软件中打开该文档，否则程序无法完成替换操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word文档2生成完毕！\n"
     ]
    }
   ],
   "source": [
    "file = docx.Document('./三行情书.docx') #打开已经存在的Word文档，只要在Document后的括号里填写Word文档的路径即可\n",
    "file.save('./三行情书2.docx')\n",
    "print('word文档2生成完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中level=0表示标题级别为0，字号较大, 不过它会默认添加下画线，可以用添加段落的方式来创建标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = docx.Document('./三行情书2.docx')\n",
    "file.add_heading('三行情书--标题', level=0)  # 添加标题\n",
    "\n",
    "file.add_picture('谷歌用户代理信息.jpg')  # 添加图片\n",
    "\n",
    "file.add_page_break()  # 添加分页符\n",
    "\n",
    "table = file.add_table(rows=1, cols=3)  # 添加表格\n",
    "table.cell(0, 0).text = '哥哥'\n",
    "table.cell(0, 1).text = '在吗'\n",
    "table.cell(0, 2).text = '喜欢你'\n",
    "\n",
    "file.save('./三行情书3.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取Word文档的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "螃蟹在剥我的壳，笔记本在写我\n",
      "漫天的我落在枫叶上雪花上\n",
      "而你在想我\n",
      "三行情书--标题\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = docx.Document('./三行情书3.docx')\n",
    "for paragraph in file.paragraphs:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23.2用Python创建Word文档的进阶知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置中文字体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目符号和编号用得相对较少，只需要在添加段落时设置参数style=’序号格式’，“List Bullet”表示项目符号，“List Number”表示数字编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档展示4生成完毕！\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "file = docx.Document()\n",
    "\n",
    "# 1、设置中文字体\n",
    "from docx.oxml.ns import qn  # 字体\n",
    "file.styles['Normal'].font.name = u'微软雅黑'\n",
    "file.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'微软雅黑')\n",
    "\n",
    "# 2、写入若干段落\n",
    "# 设置字体颜色和大小\n",
    "p = file.add_paragraph()\n",
    "run = p.add_run('螃蟹在剥我的壳，笔记本在写我')\n",
    "font = run.font \n",
    "\n",
    "from docx.shared import Pt  # 大小\n",
    "font.size = Pt(26)  # 设置字体大小\n",
    "\n",
    "from docx.shared import RGBColor  # 颜色\n",
    "font.color.rgb = RGBColor(54, 95, 145) # 设置字体颜色\n",
    "\n",
    "# 设置字体粗体、斜体、下划线\n",
    "p = file.add_paragraph()\n",
    "run = p.add_run('漫天的我落在枫叶上雪花上')\n",
    "font = run.font \n",
    "font.italic = True  # 设置斜体\n",
    "font.underline = True # 设置字下划线\n",
    "font.bold = True  # 设置粗体\n",
    "\n",
    "# 3、设置对齐方式\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH # 段落对齐\n",
    "p = file.add_paragraph()\n",
    "p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY  # 两端对齐\n",
    "p.add_run('而你在想我')\n",
    "\n",
    "# 4、设置首行缩进\n",
    "from docx.shared import Inches # 设置首行缩进\n",
    "p = file.add_paragraph()\n",
    "p.paragraph_format.first_line_indent = Inches(0.32)\n",
    "p.add_run('我也在想你')\n",
    "\n",
    "# 5、设置行距\n",
    "from docx.shared import Pt \n",
    "p = file.add_paragraph()\n",
    "p.paragraph_format.line_spacing = Pt(16)  # 设置行距\n",
    "p.add_run('行间距呀') \n",
    "\n",
    "# 6、设置段前段后距离\n",
    "from docx.shared import Pt \n",
    "p = file.add_paragraph()\n",
    "p.paragraph_format.space_before = Pt(14)\n",
    "p.paragraph_format.space_after = Pt(14)\n",
    "p.add_run('段前段后距离展示')\n",
    "\n",
    "# 7、设置项目编号\n",
    "file.add_paragraph('项目符号', style = 'List Bullet') \n",
    "file.add_paragraph('数字编号', style = 'List Number') \n",
    "\n",
    "file.add_page_break()\n",
    "\n",
    "table = file.add_table(rows=2, cols=3, style = 'Light Shading Accent 1')\n",
    "table.cell(0,0).text = '第一句'\n",
    "table.cell(0,1).text = '第二句'\n",
    "table.cell(0,2).text = '第三句'\n",
    "table.cell(1,0).text = '在吗'\n",
    "table.cell(1,1).text = '克制'\n",
    "table.cell(1,2).text = '再克制'\n",
    "\n",
    "file.add_paragraph('\\n')\n",
    "\n",
    "from docx.shared import Inches\n",
    "file.add_picture('谷歌用户代理信息.jpg', width=Inches(3), height=Inches(3))\n",
    "last_paragraph = file.paragraphs[-1]\n",
    "last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "file.save('./文档展示4.docx')\n",
    "print('文档展示4生成完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样式1:Table Grid，这个是很常见的黑白边框样式。样式2:Light Shading，它在样式1的基础上每隔一行就设置灰色的底色。样式3:Light Shading Accent 1，这个就是案例中使用的表格样式，为蓝白相间的行，可以把数字1换成2、3、4、5、6，颜色便会变为红、绿、紫、青、橙。样式4:Light List，其表头的底色为黑色，表身的底色为白色。若写成LightList Accent 1，则表头的底色变为蓝色，把数字1换成2、3、4、5、6，底色会变为红、绿、紫、青、橙。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24、自动生成数据分析报告Word文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx  # 生成文档对象\n",
    "from docx.shared import RGBColor  # 设置字体颜色\n",
    "from docx.shared import Inches  # 设置首行缩进\n",
    "from docx.shared import Pt  # 设置行距、段前段后距离\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH  # 设置段落对齐\n",
    "from docx.oxml.ns import qn  # 设置字体\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个空白word文档，并设置好字体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = docx.Document()\n",
    "file.styles['Normal'].font.name = u'微软雅黑'\n",
    "file.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'微软雅黑')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个封面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = file.add_paragraph()\n",
    "p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "p.paragraph_format.space_before = Pt(200)\n",
    "p.paragraph_format.space_after = Pt(30)\n",
    "run = p.add_run('华小智数据分析报告')\n",
    "font = run.font\n",
    "font.color.rgb = RGBColor(54, 95, 145)\n",
    "font.size = Pt(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封面上创建日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = time.strftime(\"Y\")\n",
    "month = time.strftime(\"m\")\n",
    "day = time.strftime(\"d\")\n",
    "today = year + '年' + month + '月' + day + '日'\n",
    "p = file.add_paragraph()\n",
    "p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "run = p.add_run(today)\n",
    "font.color.rgb = RGBColor(54, 95, 145)\n",
    "font.size = Pt(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0x13f54c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.add_page_break()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = file.add_paragraph()\n",
    "p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "run = p.add_run('第一部分 阿里巴巴分析报告')\n",
    "font = run.font\n",
    "font.color.rgb = RGBColor(54, 95, 145)\n",
    "font.size = Pt(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "db = pymysql.connect(\n",
    "    host = 'localhost',\n",
    "    port = 3306,\n",
    "    user = 'root',\n",
    "    password = '',\n",
    "    database = 'pachong',\n",
    "    charset = 'utf8'\n",
    ")\n",
    "company = '阿里巴巴'\n",
    "# today = time.strftime('%Y-%m-%d')\n",
    "cur = db.cursor()\n",
    "sql = 'SELECT * FROM xinwen WHERE company=%s;'\n",
    "cur.execute(sql,company)\n",
    "data = cur.fetchall()\n",
    "db.commit()\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写正文引言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0x1840f70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = len(data)\n",
    "p = file.add_paragraph()\n",
    "p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "p.paragraph_format.first_line_indent = Inches(0.32)\n",
    "introduction = '本次数据挖掘监控对象为阿里巴巴，主要爬取网站为百度新闻，当天共爬取新闻'+str(num)+'篇，具体新闻如下：'\n",
    "p.add_run(introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写具体新闻内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num):\n",
    "    p = file.add_paragraph()\n",
    "    p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    p.add_run(str(i+1)+'.'+data[i][1])  #新闻标题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "插入表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = file.add_table(rows=num+1,cols=3,style='Light Shading Accent 1')\n",
    "table.cell(0,0).text = '公司'\n",
    "table.cell(0,1).text = '标题'\n",
    "table.cell(0,2).text = '来源'\n",
    "for i in range(num):\n",
    "    table.cell(i+1,0).text = '阿里巴巴'\n",
    "    table.cell(i+1,1).text = data[i][1]\n",
    "    table.cell(i+1,2).text = data[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "报告生成完毕！\n"
     ]
    }
   ],
   "source": [
    "file.save('./华小智数据分析报告.docx')\n",
    "print('报告生成完毕！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('华小智数据分析报告.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25、基于股票信息及其衍生变量的数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     2\n",
       "2     3\n",
       "3     4\n",
       "4     5\n",
       "5     6\n",
       "6     7\n",
       "7     8\n",
       "8     9\n",
       "9    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series([1,2,3,4,5,6,7,8,9,10])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     NaN\n",
       "3     NaN\n",
       "4    15.0\n",
       "5    20.0\n",
       "6    25.0\n",
       "7    30.0\n",
       "8    35.0\n",
       "9    40.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.rolling(5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     6.0\n",
       "3    10.0\n",
       "4    15.0\n",
       "5    20.0\n",
       "6    25.0\n",
       "7    30.0\n",
       "8    35.0\n",
       "9    40.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.rolling(5,min_periods=3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code = '000002'\n",
    "stock_name = '万科A'\n",
    "start_date = '2019-02-01'\n",
    "end_date = '2019-04-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_k = ts.get_hist_data(stock_code, start = start_date, end = end_date)\n",
    "stock_table = pd.DataFrame()\n",
    "for current_date in stock_k.index:\n",
    "    current_k_line = stock_k.loc[current_date]\n",
    "    df = ts.get_tick_data(stock_code, date = current_date, src = 'tt')\n",
    "    \n",
    "    df['time'] = pd.to_datetime(current_date + '' + df['time'])\n",
    "    t = pd.to_datetime(current_date).replace(hour = 9, minute = 40)\n",
    "    df_10 = df[df.time < t]\n",
    "    vol = df_10.volume.sum()\n",
    "    \n",
    "    current_stock_info = {\n",
    "        '名称':stock_name,\n",
    "        '日期':pd.to_datetime(current_date),\n",
    "        '开盘价':current_k_line.open,\n",
    "        '收盘价':current_k_line.close,\n",
    "        '股票涨跌幅':current_k_line.p_change,\n",
    "        '10分钟成交量':vol\n",
    "    }\n",
    "    stock_table = stock_table.append(current_stock_info, ignore_index = True)\n",
    "    stock_table['10分钟成交量10日均值'] = stock_table['10分钟成交量'].sort_index.rolling(10, min_periods = 1).mean()\n",
    "    stock_table['10分钟成交量涨跌幅'] = (stock_table['10分钟成交量'] - stock_table['10分钟成交量10日均值']) / stock_table['10分钟成交量10日均值']\n",
    "    target_columns = ['名称', '开盘价', '收盘价', '股票涨跌幅', '10分钟成交量', '10分钟成交量涨跌幅']\n",
    "    final_table = stock_table[target_columns]\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# df = ts.get_tick_data('000002', date = '2019-02-11', src = 'tt')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-02-11 09:40:00')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = pd.to_datetime('2019-02-11').replace(hour = 9, minute = 40)\n",
    "# t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-02-11 00:00:00')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.to_datetime('2019-02-11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据可视化呈现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 中文标签\n",
    "plt.rcParams['axes.unicode-minus'] = False  # 负号\n",
    "\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26、用xlwings库生成Excel工作簿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过pandas库生成Excel工作簿，这种方式局限性较大，如无法选择具体的单元格。若要生成一个内容丰富的Excel工作簿，还要用到其他第三方库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\16408\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting xlwings\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/cc/6e/e24507c9cb36854e9143bf6929eb0e43165b052ca2d22febdf7f0550e991/xlwings-0.24.3.tar.gz (784 kB)\n",
      "Requirement already satisfied: pywin32>=224 in c:\\users\\16408\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from xlwings) (228)\n",
      "Using legacy 'setup.py install' for xlwings, since package 'wheel' is not installed.\n",
      "Installing collected packages: xlwings\n",
      "    Running setup.py install for xlwings: started\n",
      "    Running setup.py install for xlwings: finished with status 'done'\n",
      "Successfully installed xlwings-0.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xlwings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "app = xw.App(visible = False) # visible参数通常设置为False，让Excel程序窗口隐藏在后台运行。\n",
    "wb = app.books.add()  # 新建excel工作表\n",
    "\n",
    "sht = wb.sheets.add('新工作表')  # 新建\n",
    "sht.range('A1').value = '华小智'  # 修改某个单元格内容\n",
    "\n",
    "wb.save('./text.xlsx')\n",
    "wb.close()\n",
    "app.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "app = xw.App(visible = False) \n",
    "wb = app.books.open('./text.xlsx') # 打开已有excel文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "操控工作表和单元格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = wb.sheets['Sheet1']  # 选取\n",
    "sht = wb.sheets.add('新工作表')  # 新建\n",
    "sht.range('A1').value = '华小智'  # 修改某个单元格内容\n",
    "a = sht.range('A1').value  # 获取某个单元格内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xlwings库与pandas库的交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "app = xw.App(visible = False) # visible参数通常设置为False，让Excel程序窗口隐藏在后台运行。\n",
    "wb = app.books.add()  # 新建excel工作表\n",
    "sht = wb.sheets.add('新工作表') \n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[1, 2], [3, 4]], columns = ['a', 'b'])\n",
    "sht.range('A1').value = df\n",
    "\n",
    "wb.save('./text2.xlsx')\n",
    "wb.close()\n",
    "app.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xlwings库与Matplotlib库的交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3个参数update设置为True，则在后续通过pictures.add()调用具有相同名称（’图片1'）的图片时，可以只更新图片数据而不更改其位置或大小。第4个参数left设置图片的位置，设置left=500表示让图片距离左侧边界500像素，同理可以设置参数top=400，让图片距离上方边界400像素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "app = xw.App(visible = False) # visible参数通常设置为False，让Excel程序窗口隐藏在后台运行。\n",
    "wb = app.books.open('./text2.xlsx')  # 新建excel工作表\n",
    "sht = wb.sheets['新工作表']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "x = [1, 2, 3]\n",
    "y = [2, 4, 6]\n",
    "plt.plot(x, y)\n",
    "sht.pictures.add(fig, name='图片1', update=True, left=500)\n",
    "\n",
    "wb.save('./text4.xlsx')\n",
    "wb.close()\n",
    "app.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27、云服务器部署实战\n",
    "本地运行是无法保证程序一直运行的，一旦计算机关机或出现故障，程序就停止运行了。要解决这个问题，可以利用云服务器进行程序的云端部署。云服务器可以24小时不关机，而且基本不会出现故障，从而可以实现程序24小时不间断运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28、机器学习之客户违约预测模型搭建\n",
    "可用于进行客户违约预测的模型有很多，如逻辑回归模型、决策树模型、神经网络模型等。这里使用的原理为机器学习中的决策树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习通过模拟或实现人类的学习行为，以探寻规律或获得新的技能，它在某种程度上可以说是人工智能的核心。\n",
    "\n",
    "机器学习在金融领域的应用有客户违约预测模型、金融反欺诈模型、客户流失预警模型、精准营销模型、股价涨跌预测模型等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28.1决策树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型是机器学习的各种算法模型中比较好理解的一个模型，其基本原理是通过对一系列问题进行if/else的推导，最终实现相关决策。商业实战中不会仅根据两个特征就判断违约或不违约，而是根据多个特征来预测违约的概率，并根据设定的阈值来判断是否违约。例如，预测的违约概率超过50%即认为该客户会违约。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28.1.1决策树模型的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型中的几个重要概念：父节点、子节点、根节点、叶子节点。父节点和子节点是相对的，子节点由父节点根据某一规则分裂而来，然后子节点作为新的父节点继续分裂，直至不能分裂为止。根节点是没有父节点的节点，即初始节点。叶子节点则是没有子节点的节点，即最后的节点。决策树模型的关键即是如何选择合适的节点进行分裂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28.1.2决策树模型的建树依据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼系数（gini）用于计算一个系统中的失序现象，即系统的混乱程度。基尼系数越高，系统混乱程度越高，建立决策树模型的目的就是通过合适的分类来降低系统的混乱程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('')\n",
    "X = df.drop(columns = '是否违约')\n",
    "y = df['是否违约']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # test_size是测试集数据所占比例\n",
    "\n",
    "from sklearn import DecisionTreeClasifier\n",
    "clf = DecisionTreeClasifier(max_depth = 3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "a = pd.DataFrame()\n",
    "a['预测值'] = list(y_pred)\n",
    "a['实际值'] = list(y_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)  # 整体的预测准确度\n",
    "print(score)\n",
    "\n",
    "# 分类决策树模型在本质上预测的并不是准确的0或1的分类，而是预测属于某一分类的概率\n",
    "y_pred_proba = clf.predict_prob(X_test)\n",
    "y_pred_proba[:,1]  # 查看不违约概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次运行程序时，train_test_split()函数都会随机划分数据，如果想让每次划分数据产生的内容都是一致的，可以设置random_state参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在商业实战中一般不会以准确度作为模型的评估标准，因为准确度很多时候并不可靠。举个例子，假设100个客户里有10个客户违约，而如果模型预测所有客户都不会违约，虽然这个模型一个违约客户都没有过滤掉，但是模型的预测准确度仍然能达到90%，显然这个较高的准确度并不能反映模型的优劣。在商业实战中，我们更关心真正率、假正率两个指标。\n",
    "\n",
    "真正率计算的是在所有实际违约的人中，预测为违约的比例，也称命中率或召回率；而假正率计算的则是在所有实际没有违约的人当中，预测为违约的比例，也称假警报率。一个优秀的客户违约预测模型，我们希望命中率（TPR）尽可能高，即能尽量揪出“坏人”，同时也希望假警报率（FPR）尽可能低，即不要误伤“好人”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两者往往成正相关性，因为一旦调高阈值，例如，认为违约概率超过90%的才认定为违约，那么会导致假警报率很低，但是命中率也很低；而降低阈值，例如，认为违约概率超过10%就认定为违约，那么命中率就会很高，但是假警报率也会很高。因此，为了衡量一个模型的优劣，数据科学家根据不同阈值下的命中率和假警报率绘制了ROC曲线图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC曲线的横坐标为假警报率（FPR），纵坐标为命中率（TPR）。在同一个阈值条件下，我们希望所搭建的模型的命中率尽可能高，而假警报率尽可能低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果把假警报率理解为代价，那么命中率就是收益，所以也可以说在同一个阈值条件下，我们希望在代价（假警报率）尽量小的情况下，收益（命中率）尽量高。该思想反映在图形上就是曲线尽可能地陡峭，曲线越靠近左上角，说明在同样的阈值条件下，命中率越高，假警报率越小，模型越完善。换一个角度来理解，一个完美的模型是在不同的阈值下，命中率都接近于1，而假警报率则接近于0。该特征反映在图形上，就是曲线非常接近（0, 1）这个点，即曲线非常陡峭。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数值比较上可以使用AUC值来衡量模型的好坏。AUC值（Area Under aCurve）指在曲线下面的面积，该面积的取值范围通常为0.5～1,0.5代表随机判断，1则代表完美的模型。在商业实战中，因为存在很多扰动因子，AUC值能达到0.75以上就已经可以接受，如果能达到0.85以上，就是非常不错的模型了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thres = roc_curve(y_test.values, y_predproba[:,1])\n",
    "a = pd.DataFrame()\n",
    "a['阈值'] = list(thres)\n",
    "a['假警报率'] = list(fpr)\n",
    "a['命中率'] = list(tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_auc_socre\n",
    "score = roc_auc_socre(y_test.values, y_predproba[:,1])\n",
    "print(score)\n",
    "\n",
    "clf.feature_importances_  # 特征变量的特征重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型作为机器学习的经典算法模型，有着独特的优势，如对异常值不敏感、可解释性强等，不过它也存在一些缺点，如结果不稳定、容易造成过拟合等。所以在商业实战中常使用一个基于决策树模型的集成算法模型——随机森林模型。随机森林模型是通过多个决策树模型共同搭建的，其结果更稳定且不易过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
